import asyncio
import itertools
import json
import ssl
import traceback
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple, Union

import asyncpg
from asyncpg import Connection

from .base_graph_store import BaseGraphStore
from .base import GraphDBConfig
from aworld.logs.util import logger


@dataclass
class PostgreSQLDB:
    """PostgreSQLæ•°æ®åº“è¿æ¥ç®¡ç†"""
    
    def __init__(self, config: dict[str, Any], **kwargs: Any):
        self.config = config
        self.pool: Optional[asyncpg.Pool] = None
        self._initialization_lock = asyncio.Lock()
        self._is_initialized = False
    
    async def initdb(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
        async with self._initialization_lock:
            if self._is_initialized:
                return
                
            try:
                # æ„å»ºè¿æ¥å‚æ•°
                connection_params = {
                    "host": self.config.get("host", "localhost"),
                    "port": self.config.get("port", 5432),
                    "user": self.config.get("user", "aworldcore"),
                    "password": self.config.get("password", "123456"),
                    "database": self.config.get("database", "aworldcore"),
                }
                
                # SSLé…ç½®
                if self.config.get("ssl_mode"):
                    connection_params["ssl"] = self._create_ssl_context()
                
                # åˆ›å»ºè¿æ¥æ± 
                self.pool = await asyncpg.create_pool(
                    **connection_params,
                    min_size=1,
                    max_size=10,
                    command_timeout=120,  # å¢åŠ å‘½ä»¤è¶…æ—¶æ—¶é—´
                    server_settings={
                        'application_name': 'aworldcore_graph_store',
                        'tcp_keepalives_idle': '600',
                        'tcp_keepalives_interval': '30',
                        'tcp_keepalives_count': '3',
                    },
                    # è¿æ¥è¶…æ—¶è®¾ç½®
                    timeout=30,  # è¿æ¥è¶…æ—¶30ç§’
                    max_queries=50000,  # æœ€å¤§æŸ¥è¯¢æ•°
                    max_inactive_connection_lifetime=300.0,  # éæ´»è·ƒè¿æ¥æœ€å¤§ç”Ÿå­˜æ—¶é—´
                )
                
                # é…ç½®AGEæ‰©å±•
                async with self.pool.acquire() as connection:
                    await self.configure_age_extension(connection)
                
                self._is_initialized = True
                    
            except Exception as e:
                self._is_initialized = False
                raise Exception(f"Failed to initialize PostgreSQL database: {e}")
    
    def _create_ssl_context(self) -> ssl.SSLContext | None:
        """åˆ›å»ºSSLä¸Šä¸‹æ–‡"""
        try:
            ssl_mode = self.config.get("ssl_mode", "prefer")
            if ssl_mode == "disable":
                return None
            
            context = ssl.create_default_context()
            if ssl_mode == "require":
                context.check_hostname = False
                context.verify_mode = ssl.CERT_NONE
            
            return context
        except Exception:
            return None
    
    @staticmethod
    async def configure_age_extension(connection: Connection) -> None:
        """é…ç½®Apache AGEæ‰©å±•"""
        try:
            # åˆ›å»ºAGEæ‰©å±•
            await connection.execute("CREATE EXTENSION IF NOT EXISTS age;")
            
            # è®¾ç½®æœç´¢è·¯å¾„
            await connection.execute("SET search_path = ag_catalog, public;")
            
        except Exception as e:
            # å¦‚æœAGEæ‰©å±•ä¸å¯ç”¨ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­
            print(f"Warning: Apache AGE extension not available: {e}")
    
    async def query(self, sql: str, params: List[Any] = None, multirows: bool = False, 
                   with_age: bool = False, graph_name: str = None) -> Union[Dict[str, Any], List[Dict[str, Any]], None]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        if not self.pool:
            raise Exception("Database pool not initialized")
        
        connection = None
        try:
            connection = await self.pool.acquire()
            # å¯¹äºæŸ¥è¯¢æ“ä½œï¼Œä½¿ç”¨åªè¯»äº‹åŠ¡
            async with connection.transaction(readonly=True):
                if with_age and graph_name:
                    # è®¾ç½®AGEæœç´¢è·¯å¾„
                    await connection.execute(f"SET search_path = ag_catalog, public;")
                
                if multirows:
                    rows = await connection.fetch(sql, *(params or []))
                    return [dict(row) for row in rows] if rows else []
                else:
                    row = await connection.fetchrow(sql, *(params or []))
                    return dict(row) if row else None
        except Exception as e:
            # ç¡®ä¿è¿æ¥è¢«æ­£ç¡®é‡Šæ”¾
            raise e
        finally:
            if connection:
                try:
                    await self.pool.release(connection)
                except Exception as release_error:
                    print(f"Warning: Error releasing connection: {release_error}")
    
    async def execute(self, sql: str, data: Dict[str, Any] = None, upsert: bool = False, 
                     ignore_if_exists: bool = False, with_age: bool = False, graph_name: str = None):
        """æ‰§è¡ŒSQLè¯­å¥"""
        if not self.pool:
            raise Exception("Database pool not initialized")
        
        connection = None
        try:
            connection = await self.pool.acquire()
            # å¼€å§‹äº‹åŠ¡
            async with connection.transaction():
                if with_age and graph_name:
                    # è®¾ç½®AGEæœç´¢è·¯å¾„
                    await connection.execute(f"SET search_path = ag_catalog, public;")
                
                try:
                    if data:
                        # Apache AGE çš„ cypher å‡½æ•°éœ€è¦å°†å‚æ•°ä½œä¸ºå•ä¸ªå­—å…¸ä¼ é€’
                        result = await connection.fetch(sql, data)
                    else:
                        result = await connection.fetch(sql)
                    
                    # è¿”å›æŸ¥è¯¢ç»“æœ
                    return [dict(row) for row in result] if result else []
                except Exception as e:
                    if ignore_if_exists and "already exists" in str(e).lower():
                        # å¿½ç•¥"å·²å­˜åœ¨"é”™è¯¯
                        pass
                    else:
                        raise e
        except Exception as e:
            # ç¡®ä¿è¿æ¥è¢«æ­£ç¡®é‡Šæ”¾
            raise e
        finally:
            if connection:
                try:
                    await self.pool.release(connection)
                except Exception as release_error:
                    print(f"Warning: Error releasing connection: {release_error}")
    
    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.pool:
            try:
                # ç­‰å¾…æ‰€æœ‰è¿æ¥å®Œæˆ
                await asyncio.sleep(0.1)
                await self.pool.close()
                self._is_initialized = False
            except Exception as e:
                print(f"Warning: Error closing database pool: {e}")
            finally:
                self.pool = None


class ClientManager:
    """å®¢æˆ·ç«¯ç®¡ç†å™¨ - æ”¹è¿›çš„èµ„æºç®¡ç†"""
    _instances: Dict[str, Any] = {"db": None, "ref_count": 0, "_lock": asyncio.Lock(), "_shutdown": False}

    @classmethod
    async def get_client(cls, config: dict[str, Any]) -> PostgreSQLDB:
        """è·å–æ•°æ®åº“å®¢æˆ·ç«¯"""
        async with cls._instances["_lock"]:
            if cls._instances["_shutdown"]:
                raise RuntimeError("ClientManager is shutting down")
                
            if cls._instances["db"] is None:
                cls._instances["db"] = PostgreSQLDB(config)
                await cls._instances["db"].initdb()
            
            cls._instances["ref_count"] += 1
            return cls._instances["db"]
    
    @classmethod
    async def reset_client(cls):
        """é‡ç½®æ•°æ®åº“å®¢æˆ·ç«¯è¿æ¥"""
        async with cls._instances["_lock"]:
            if cls._instances["db"]:
                try:
                    await cls._instances["db"].close()
                except Exception as e:
                    logger.warning(f"Error closing database client during reset: {e}")
                finally:
                    cls._instances["db"] = None
                    cls._instances["ref_count"] = 0
    
    @classmethod
    async def release_client(cls, db: PostgreSQLDB):
        """é‡Šæ”¾æ•°æ®åº“å®¢æˆ·ç«¯"""
        async with cls._instances["_lock"]:
            if cls._instances["ref_count"] > 0:
                cls._instances["ref_count"] -= 1
                
            if cls._instances["ref_count"] <= 0 and cls._instances["db"]:
                try:
                    await cls._instances["db"].close()
                except Exception as e:
                    print(f"Warning: Error closing database client: {e}")
                finally:
                    cls._instances["db"] = None
                    cls._instances["ref_count"] = 0
    
    @classmethod
    async def shutdown(cls):
        """å¼ºåˆ¶å…³é—­æ‰€æœ‰è¿æ¥"""
        async with cls._instances["_lock"]:
            cls._instances["_shutdown"] = True
            if cls._instances["db"]:
                try:
                    await cls._instances["db"].close()
                except Exception as e:
                    print(f"Warning: Error during shutdown: {e}")
                finally:
                    cls._instances["db"] = None
                    cls._instances["ref_count"] = 0


@dataclass
class PGGraphStore(BaseGraphStore):
    """PostgreSQLå›¾å­˜å‚¨å®ç°"""

    graph_db_config: GraphDBConfig = field(default=None)
    db: Optional[PostgreSQLDB] = field(default=None)
    graph_name: str = field(default="")
    
    def __init__(self, graph_db_config: dict[str, Any], graph_name: str = "aworld"):
        self.graph_db_config = graph_db_config
        self.graph_name = graph_name

    async def initialize(self):
        """åˆå§‹åŒ–å›¾å­˜å‚¨"""
        if self.db is None:
            self.db = await ClientManager.get_client(self.graph_db_config)
        
        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        if not self.db._is_initialized:
            await self.db.initdb()
        
        # æ£€æŸ¥è¿æ¥å¥åº·çŠ¶æ€
        await self._check_connection_health()
        
        # åˆ›å»ºAGEæ‰©å±•å’Œé…ç½®å›¾ç¯å¢ƒ
        async with self.db.pool.acquire() as connection:
            await PostgreSQLDB.configure_age_extension(connection)
        
        # æ‰§è¡Œå›¾åˆå§‹åŒ–è¯­å¥
        queries = [
            f"SELECT create_graph('{self.graph_name}')",
            f"SELECT create_vlabel('{self.graph_name}', 'base');",
            f"SELECT create_elabel('{self.graph_name}', 'DIRECTED');",
        ]
        
        for query in queries:
            try:
                await self.db.execute(query, with_age=True, graph_name=self.graph_name, ignore_if_exists=True)
            except Exception as e:
                # å¿½ç•¥"å·²å­˜åœ¨"é”™è¯¯
                if "already exists" not in str(e).lower():
                    print(f"Warning: Failed to execute query {query}: {e}")
    
    async def _check_connection_health(self):
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥å¥åº·çŠ¶æ€"""
        try:
            if self.db and self.db.pool:
                # å°è¯•è·å–ä¸€ä¸ªè¿æ¥å¹¶æ‰§è¡Œç®€å•æŸ¥è¯¢
                async with self.db.pool.acquire() as connection:
                    await connection.fetchval("SELECT 1")
                logger.debug("Database connection health check passed")
            else:
                raise Exception("Database pool not available")
        except Exception as e:
            logger.error(f"Database connection health check failed: {e}")
            # å¦‚æœå¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–è¿æ¥
            if self.db:
                try:
                    await self.db.close()
                except:
                    pass
                self.db = None
                # é‡æ–°è·å–å®¢æˆ·ç«¯
                self.db = await ClientManager.get_client(self.graph_db_config)
                await self.db.initdb()
    
    async def finalize(self):
        """æ¸…ç†èµ„æº"""
        if self.db is not None:
            try:
                await ClientManager.release_client(self.db)
            except Exception as e:
                logger.warning(f"Error releasing database client: {e}")
            finally:
                self.db = None
    
    @staticmethod
    def _record_to_dict(record: asyncpg.Record) -> Dict[str, Any]:
        """å°†AGEæŸ¥è¯¢è®°å½•è½¬æ¢ä¸ºå­—å…¸"""
        d = {}
        
        for k in record.keys():
            v = record[k]
            if isinstance(v, str) and "::" in v:
                # å¤„ç†AGEç±»å‹æ•°æ®
                if v.startswith("[") and v.endswith("]"):
                    # å¤„ç†æ•°ç»„ç±»å‹
                    json_content = v[:v.rfind("::")]
                    type_id = v[v.rfind("::") + 2:]
                    if type_id in ["vertex", "edge"]:
                        try:
                            parsed_data = json.loads(json_content)
                            d[k] = parsed_data
                        except json.JSONDecodeError:
                            d[k] = None
                else:
                    # å¤„ç†å•ä¸ªå¯¹è±¡
                    json_content = v[:v.rfind("::")]
                    type_id = v[v.rfind("::") + 2:]
                    if type_id in ["vertex", "edge"]:
                        try:
                            parsed_data = json.loads(json_content)
                            d[k] = parsed_data
                        except json.JSONDecodeError:
                            d[k] = None
                    else:
                        d[k] = v
            else:
                d[k] = v
        
        return d
    
    @staticmethod
    def _format_properties(properties: Dict[str, Any], _id: Optional[str] = None) -> str:
        """å°†å±æ€§å­—å…¸è½¬æ¢ä¸ºCypheræŸ¥è¯¢å­—ç¬¦ä¸²"""
        props = []
        for k, v in properties.items():
            prop = f"`{k}`: {json.dumps(v)}"
            props.append(prop)
        
        if _id is not None and "id" not in properties:
            props.append(f"id: {json.dumps(_id)}")
        
        return "{" + ", ".join(props) + "}"
    
    async def _query(self, query: str, readonly: bool = True, upsert: bool = False, 
                    params: Dict[str, Any] = None, max_retries: int = 5) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå›¾æŸ¥è¯¢"""
        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        if self.db is None:
            await self.initialize()
        
        for attempt in range(max_retries):
            try:
                if readonly:
                    data = await self.db.query(
                        query,
                        list(params.values()) if params else None,
                        multirows=True,
                        with_age=True,
                        graph_name=self.graph_name,
                    )
                else:
                    data = await self.db.execute(
                        query,
                        params,
                        upsert=upsert,
                        with_age=True,
                        graph_name=self.graph_name,
                    )
                
                if data is None:
                    return []
                else:
                    return [self._record_to_dict(d) for d in data]
            
            except Exception as e:
                error_msg = str(e).lower()
                # æ£€æŸ¥æ˜¯å¦æ˜¯è¿æ¥ç›¸å…³çš„é”™è¯¯ï¼Œéœ€è¦é‡è¯•
                retryable_errors = [
                    "another operation is in progress",
                    "connection is closed",
                    "connection lost",
                    "connection timeout",
                    "connection reset",
                    "timeout",
                    "cancelled",
                    "pool is closed",
                    "connection pool is closed"
                ]
                
                is_retryable = any(keyword in error_msg for keyword in retryable_errors)
                
                if is_retryable and attempt < max_retries - 1:
                    # æŒ‡æ•°é€€é¿é‡è¯•ï¼Œå¢åŠ ç­‰å¾…æ—¶é—´
                    wait_time = min(2.0 * (2 ** attempt), 30.0)  # æœ€å¤§ç­‰å¾…30ç§’
                    logger.warning(f"Database connection error, retrying ({attempt + 1}/{max_retries}) in {wait_time:.1f}s: {e}")
                    await asyncio.sleep(wait_time)
                    
                    # å¦‚æœæ˜¯è¿æ¥æ± é—®é¢˜ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–
                    if "pool" in error_msg and attempt == 1:
                        try:
                            logger.info("Attempting to reinitialize database connection pool...")
                            await self.db.close()
                            self.db = None
                            await self.initialize()
                        except Exception as reinit_error:
                            logger.error(f"Failed to reinitialize database pool: {reinit_error} {traceback.format_exc()}")
                    
                    continue
                else:
                    logger.error(f"Database query failed after {attempt + 1} attempts: {e} {traceback.format_exc()}")
                    raise Exception(f"Error executing graph query: {query}, error: {e}")
 
    async def has_node(self, namespace, node_id: str) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨"""
        query = f"""
            SELECT EXISTS (
              SELECT 1
              FROM {self.graph_name}.base
              WHERE ag_catalog.agtype_access_operator(
                      VARIADIC ARRAY[properties, '"id"'::agtype]
                    ) = (to_json($1::text)::text)::agtype
                AND ag_catalog.agtype_access_operator(
                      VARIADIC ARRAY[properties, '"namespace"'::agtype]
                    ) = (to_json($2::text)::text)::agtype
              LIMIT 1
            ) AS node_exists;
        """
        
        params = {"node_id": node_id, "namespace": namespace}
        row = (await self._query(query, params=params))[0]
        return bool(row["node_exists"])

    async def has_edge(self, namespace, source_node_id: str, target_node_id: str) -> bool:
        """æ£€æŸ¥è¾¹æ˜¯å¦å­˜åœ¨"""
        query = f"""
            WITH a AS (
              SELECT id AS vid
              FROM {self.graph_name}.base
              WHERE ag_catalog.agtype_access_operator(
                      VARIADIC ARRAY[properties, '"id"'::agtype]
                    ) = (to_json($1::text)::text)::agtype
                AND ag_catalog.agtype_access_operator(
                      VARIADIC ARRAY[properties, '"namespace"'::agtype]
                    ) = (to_json($3::text)::text)::agtype
            ),
            b AS (
              SELECT id AS vid
              FROM {self.graph_name}.base
              WHERE ag_catalog.agtype_access_operator(
                      VARIADIC ARRAY[properties, '"id"'::agtype]
                    ) = (to_json($2::text)::text)::agtype
                AND ag_catalog.agtype_access_operator(
                      VARIADIC ARRAY[properties, '"namespace"'::agtype]
                    ) = (to_json($3::text)::text)::agtype
            )
            SELECT EXISTS (
              SELECT 1
              FROM {self.graph_name}."DIRECTED" d
              JOIN a ON d.start_id = a.vid
              JOIN b ON d.end_id   = b.vid
              LIMIT 1
            ) AS edge_exists;
        """
        
        params = {"source_node_id": source_node_id, "target_node_id": target_node_id, "namespace": namespace}
        row = (await self._query(query, params=params))[0]
        return bool(row["edge_exists"])

    async def get_node(self, namespace, node_id: str) -> Optional[dict[str, str]]:
        """è·å–èŠ‚ç‚¹"""
        result = await self.get_nodes_batch(namespace=namespace, node_ids=[node_id])
        if result and node_id in result:
            return result[node_id]
        return None

    
    async def get_edge(self, namespace, source_node_id: str, target_node_id: str) -> Optional[Dict[str, Any]]:
        """è·å–è¾¹"""
        result = await self.get_edges_batch(namespace, [{"src": source_node_id, "tgt": target_node_id}])
        if result and (source_node_id, target_node_id) in result:
            return result[(source_node_id, target_node_id)]
        return None

    async def upsert_node(self, namespace, node_id: str, node_data: dict[str, str]) -> None:
        # ç¡®ä¿namespaceå±æ€§è¢«åŒ…å«åœ¨èŠ‚ç‚¹æ•°æ®ä¸­
        node_data_with_namespace = node_data.copy()
        node_data_with_namespace['namespace'] = namespace
        properties = self._format_properties(node_data_with_namespace)

        query = """SELECT * FROM cypher('%s', $$
                         MERGE (n:base {id: "%s"})
                         SET n += %s
                         RETURN n
                       $$) AS (n agtype)""" % (
            self.graph_name,
            node_id,
            properties,
        )

        try:
            # ç¡®ä¿æ•°æ®åº“è¿æ¥å¥åº·
            await self._check_connection_health()
            
            await self._query(query, readonly=False, upsert=True)
            logger.debug(f"Successfully upserted node: {node_id}")

        except Exception as e:
            logger.error(
                f"[{self.graph_name}] POSTGRES, upsert_node error on node_id: `{node_id}`, error: {e}"
            )
            
            # å¦‚æœæ˜¯è¿æ¥è¶…æ—¶é”™è¯¯ï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–è¿æ¥
            error_msg = str(e).lower()
            if any(keyword in error_msg for keyword in ["timeout", "connection", "cancelled"]):
                logger.warning(f"Connection issue detected for node {node_id}, attempting to recover...")
                try:
                    # é‡ç½®å®¢æˆ·ç«¯è¿æ¥
                    await ClientManager.reset_client()
                    self.db = None
                    await self.initialize()
                    
                    # é‡è¯•ä¸€æ¬¡
                    await self._query(query, readonly=False, upsert=True)
                    logger.info(f"Successfully recovered and upserted node: {node_id}")
                    return
                except Exception as retry_error:
                    logger.error(f"Failed to recover connection for node {node_id}: {retry_error}")
            
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä½†æ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
            raise Exception(f"Failed to upsert node {node_id}: {e}") from e

    async def upsert_edge(self, namespace, source_node_id: str, target_node_id: str, edge_data: Dict[str, Any]) -> None:
        try:
            """æ’å…¥æˆ–æ›´æ–°è¾¹"""
            # ç¡®ä¿æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹å­˜åœ¨
            source = await self.get_node(source_node_id)
            target = await self.get_node(target_node_id)
            if not source or not target:
                raise ValueError(f"Source or target node does not exist: {source_node_id}, {target_node_id}")

            # ç¡®ä¿è¾¹æ•°æ®åŒ…å«namespaceå±æ€§
            edge_data_with_namespace = edge_data.copy()
            edge_data_with_namespace['namespace'] = namespace
            edge_properties = self._format_properties(edge_data_with_namespace)

            query = """SELECT * FROM cypher('%s', $$
                         MATCH (source:base {id: "%s"})
                         WITH source
                         MATCH (target:base {id: "%s"})
                         MERGE (source)-[r:DIRECTED]-(target)
                         SET r += %s
                         RETURN r
                       $$) AS (r agtype)""" % (
                self.graph_name,
                source_node_id,
                target_node_id,
                edge_properties,
            )
            await self._query(query, readonly=False, upsert=True)
        except Exception:
            logger.error(
                f"[{self.graph_name}] POSTGRES, upsert_edge error on edge: `{source_node_id}`-`{target_node_id}` {traceback.format_exc()}"
            )
            raise

    async def remove_nodes(self, namespace, node_ids: List[str] = None) -> None:
        """åˆ é™¤å¤šä¸ªèŠ‚ç‚¹"""
        if not node_ids:
            return
        
        # æ„å»ºèŠ‚ç‚¹IDåˆ—è¡¨çš„å­—ç¬¦ä¸²
        node_ids_str = ", ".join([f"'{node_id}'" for node_id in node_ids])
        
        query = f"""
            SELECT * FROM cypher('{self.graph_name}', $$
                MATCH (n:base)
                WHERE n.id IN [{node_ids_str}]
                DETACH DELETE n
                RETURN count(n) as deleted_count
            $$) AS (result agtype);
        """
        
        result = await self._query(query, readonly=False)
        if result:
            deleted_count = result[0].get('result', {}).get('deleted_count', 0)
            logger.info(f"ğŸ—‘ï¸ Successfully deleted {deleted_count} nodes: {node_ids}")
        
        # éªŒè¯åˆ é™¤æ˜¯å¦æˆåŠŸ
        await self._verify_nodes_deleted(namespace, node_ids)

    async def remove_edges(self, namespace, edges: List[Tuple[str, str]]) -> None:
        """åˆ é™¤å¤šä¸ªè¾¹"""
        if not edges:
            return
        
        deleted_count = 0
        # ä¸ºæ¯ä¸ªè¾¹æ„å»ºåˆ é™¤æŸ¥è¯¢
        for source_id, target_id in edges:
            query = f"""
                SELECT * FROM cypher('{self.graph_name}', $$
                    MATCH (a:base {{id: $source_id}})-[r:DIRECTED]->(b:base {{id: $target_id}})
                    DELETE r
                    RETURN count(r) as deleted_count
                $$, $1) AS (result agtype);
            """
            
            params = {
                "source_id": source_id,
                "target_id": target_id
            }
            
            result = await self._query(query, readonly=False, params=params)
            if result and result[0].get('result', {}).get('deleted_count', 0) > 0:
                deleted_count += 1
        
        logger.info(f"ğŸ—‘ï¸ Successfully deleted {deleted_count} edges out of {len(edges)}")
        
        # éªŒè¯åˆ é™¤æ˜¯å¦æˆåŠŸ
        await self._verify_edges_deleted(edges)

    async def get_nodes_batch(self, namespace, node_ids: List[str], batch_size: int = 1000) -> Dict[str, Dict[str, Any]]:
        """æ‰¹é‡è·å–èŠ‚ç‚¹"""
        if not node_ids:
            return {}

        nodes_dict = {}
        
        # åˆ†æ‰¹å¤„ç†èŠ‚ç‚¹ID
        for i in range(0, len(node_ids), batch_size):
            batch = node_ids[i:i + batch_size]
            
            # æ„å»ºèŠ‚ç‚¹IDåˆ—è¡¨çš„å­—ç¬¦ä¸²
            node_ids_str = ", ".join([f"'{node_id}'" for node_id in batch])

            query = f"""
                WITH input(v, ord) AS (
                  SELECT v, ord
                  FROM unnest($1::text[]) WITH ORDINALITY AS t(v, ord)
                ),
                ids(node_id, ord) AS (
                  SELECT (to_json(v)::text)::agtype AS node_id, ord
                  FROM input
                )
                SELECT i.node_id::text AS node_id,
                       b.properties
                FROM {self.graph_name}.base AS b
                JOIN ids i
                  ON ag_catalog.agtype_access_operator(
                       VARIADIC ARRAY[b.properties, '"id"'::agtype]
                     ) = i.node_id
                WHERE ag_catalog.agtype_access_operator(
                       VARIADIC ARRAY[b.properties, '"namespace"'::agtype]
                     ) = (to_json($2::text)::text)::agtype
                ORDER BY i.ord;
            """

            results = await self._query(query, params={"ids": batch, "namespace": namespace})

            for result in results:
                if result["node_id"] and result["properties"]:
                    node_dict = result["properties"]

                    # Process string result, parse it to JSON dictionary
                    if isinstance(node_dict, str):
                        try:
                            node_dict = json.loads(node_dict)
                        except json.JSONDecodeError:
                            logger.warning(
                                f"Failed to parse node string in batch: {node_dict}"
                            )

                    nodes_dict[result["node_id"]] = node_dict
        
        return nodes_dict

    async def get_all_nodes(self, namespace) -> list[dict]:
        """Get all nodes in the graph.

        Returns:
            A list of all nodes, where each node is a dictionary of its properties
        """
        query = f"""SELECT * FROM cypher('{self.graph_name}', $$
                     MATCH (n:base)
                     WHERE n.namespace = '{namespace}'
                     RETURN n
                   $$) AS (n agtype)"""

        results = await self._query(query)
        nodes = []
        for result in results:
            if result["n"]:
                node_dict = result["n"]["properties"]

                # Process string result, parse it to JSON dictionary
                if isinstance(node_dict, str):
                    try:
                        node_dict = json.loads(node_dict)
                    except json.JSONDecodeError:
                        logger.warning(
                            f"[{self.workspace}] Failed to parse node string: {node_dict}"
                        )

                # Add node id (entity_id) to the dictionary for easier access
                node_dict["id"] = node_dict.get("id")
                nodes.append(node_dict)
        return nodes

    async def get_edges_batch(self, namespace, pairs: List[Dict[str, str]], batch_size: int = 500) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """
        Retrieve edge properties for multiple (src, tgt) pairs in one query.
        Get forward and backward edges seperately and merge them before return

        Args:
            pairs: List of dictionaries, e.g. [{"src": "node1", "tgt": "node2"}, ...]
            batch_size: Batch size for the query

        Returns:
            A dictionary mapping (src, tgt) tuples to their edge properties.
        """
        if not pairs:
            return {}

        seen = set()
        uniq_pairs: list[dict[str, str]] = []
        for p in pairs:
            s = p["src"]
            t = p["tgt"]
            key = (s, t)
            if s and t and key not in seen:
                seen.add(key)
                uniq_pairs.append(p)

        edges_dict: dict[tuple[str, str], dict] = {}

        for i in range(0, len(uniq_pairs), batch_size):
            batch = uniq_pairs[i : i + batch_size]

            pairs = [{"src": p["src"], "tgt": p["tgt"]} for p in batch]

            forward_cypher = f"""
                         UNWIND $pairs AS p
                         WITH p.src AS src_eid, p.tgt AS tgt_eid
                         MATCH (a:base {{id: src_eid}})
                         MATCH (b:base {{id: tgt_eid}})
                         MATCH (a)-[r]->(b)
                         RETURN src_eid AS source, tgt_eid AS target, properties(r) AS edge_properties"""
            backward_cypher = f"""
                         UNWIND $pairs AS p
                         WITH p.src AS src_eid, p.tgt AS tgt_eid
                         MATCH (a:base {{id: src_eid}})
                         MATCH (b:base {{id: tgt_eid}})
                         MATCH (a)<-[r]-(b)
                         RETURN src_eid AS source, tgt_eid AS target, properties(r) AS edge_properties"""

            def dollar_quote(s: str, tag_prefix="AGE"):
                s = "" if s is None else str(s)
                for i in itertools.count(1):
                    tag = f"{tag_prefix}{i}"
                    wrapper = f"${tag}$"
                    if wrapper not in s:
                        return f"{wrapper}{s}{wrapper}"

            sql_fwd = f"""
            SELECT * FROM cypher({dollar_quote(self.graph_name)}::name,
                                 {dollar_quote(forward_cypher)}::cstring,
                                 $1::agtype)
              AS (source text, target text, edge_properties agtype)
            """

            sql_bwd = f"""
            SELECT * FROM cypher({dollar_quote(self.graph_name)}::name,
                                 {dollar_quote(backward_cypher)}::cstring,
                                 $1::agtype)
              AS (source text, target text, edge_properties agtype)
            """

            pg_params = {"params": json.dumps({"pairs": pairs}, ensure_ascii=False)}

            forward_results = await self._query(sql_fwd, params=pg_params)
            backward_results = await self._query(sql_bwd, params=pg_params)

            for result in forward_results:
                if result["source"] and result["target"] and result["edge_properties"]:
                    edge_props = result["edge_properties"]

                    # Process string result, parse it to JSON dictionary
                    if isinstance(edge_props, str):
                        try:
                            edge_props = json.loads(edge_props)
                        except json.JSONDecodeError:
                            logger.warning(
                                f"Failed to parse edge properties string: {edge_props}"
                            )
                            continue

                    edges_dict[(result["source"], result["target"])] = edge_props

            for result in backward_results:
                if result["source"] and result["target"] and result["edge_properties"]:
                    edge_props = result["edge_properties"]

                    # Process string result, parse it to JSON dictionary
                    if isinstance(edge_props, str):
                        try:
                            edge_props = json.loads(edge_props)
                        except json.JSONDecodeError:
                            logger.warning(
                                f"Failed to parse edge properties string: {edge_props}"
                            )
                            continue

                    edges_dict[(result["source"], result["target"])] = edge_props

        return edges_dict

    async def index_done_callback(self) -> None:
        """ç´¢å¼•å®Œæˆå›è°ƒ"""
        # PostgreSQLè‡ªåŠ¨å¤„ç†æŒä¹…åŒ–
        pass
    
    async def _flush_database_cache(self) -> None:
        """åˆ·æ–°æ•°æ®åº“ç¼“å­˜ï¼Œç¡®ä¿åˆ é™¤æ“ä½œç«‹å³ç”Ÿæ•ˆ"""
        try:
            async with self.db.pool.acquire() as connection:
                # å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç¼“å­˜
                await connection.execute("SELECT pg_stat_reset();")
                # åˆ·æ–°AGEå›¾ç¼“å­˜
                await connection.execute(f"SET search_path = ag_catalog, public;")
                # æ‰§è¡Œä¸€ä¸ªç®€å•çš„æŸ¥è¯¢æ¥åˆ·æ–°è¿æ¥
                await connection.fetch("SELECT 1;")
                logger.debug("ğŸ”„ Database cache flushed")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to flush database cache: {e}")
    
    async def _verify_nodes_deleted(self, namespace, node_ids: List[str]) -> None:
        """éªŒè¯èŠ‚ç‚¹æ˜¯å¦å·²æˆåŠŸåˆ é™¤"""
        try:
            for node_id in node_ids:
                exists = await self.has_node(namespace, node_id)
                if exists:
                    logger.warning(f"âš ï¸ Node {node_id} still exists after deletion attempt")
                else:
                    logger.debug(f"âœ… Node {node_id} successfully deleted")
        except Exception as e:
            logger.error(f"âŒ Error verifying node deletion: {e}")
    
    async def _verify_edges_deleted(self, namespace, edges: List[Tuple[str, str]]) -> None:
        """éªŒè¯è¾¹æ˜¯å¦å·²æˆåŠŸåˆ é™¤"""
        try:
            for source_id, target_id in edges:
                exists = await self.has_edge(namespace=namespace, source_node_id=source_id, target_node_id=target_id)
                if exists:
                    logger.warning(f"âš ï¸ Edge {source_id}->{target_id} still exists after deletion attempt")
                else:
                    logger.debug(f"âœ… Edge {source_id}->{target_id} successfully deleted")
        except Exception as e:
            logger.error(f"âŒ Error verifying edge deletion: {e}")


    async def get_node_edges(self, namespace, source_node_id: str) -> Optional[List[Tuple[str, str]]]:
        """
        Retrieves all edges (relationships) for a particular node identified by its label.
        :return: list of dictionaries containing edge information
        """

        query = """SELECT * FROM cypher('%s', $$
                      MATCH (n:base {id: "%s"})
                      OPTIONAL MATCH (n)-[]-(connected:base)
                      RETURN n.id AS source_id, connected.id AS connected_id
                    $$) AS (source_id text, connected_id text)""" % (
            self.graph_name,
            source_node_id,
        )

        results = await self._query(query)
        edges = []
        for record in results:
            source_id = record["source_id"]
            connected_id = record["connected_id"]

            if source_id and connected_id:
                edges.append((source_id, connected_id))

        return edges

    async def get_nodes_edges_batch(self, namespace, node_ids: List[str], batch_size: int = 500) -> Dict[str, List[Tuple[str, str]]]:
        """
        Get all edges (both outgoing and incoming) for multiple nodes in a single batch operation.

        Args:
            node_ids: List of node IDs to get edges for
            batch_size: Batch size for the query

        Returns:
            Dictionary mapping node IDs to lists of (source, target) edge tuples
        """
        if not node_ids:
            return {}

        seen = set()
        unique_ids: list[str] = []
        for nid in node_ids:
            n = nid
            if n and n not in seen:
                seen.add(n)
                unique_ids.append(n)

        edges_norm: dict[str, list[tuple[str, str]]] = {n: [] for n in unique_ids}

        for i in range(0, len(unique_ids), batch_size):
            batch = unique_ids[i : i + batch_size]
            # Format node IDs for the query
            formatted_ids = ", ".join([f'"{n}"' for n in batch])

            outgoing_query = """SELECT * FROM cypher('%s', $$
                         UNWIND [%s] AS node_id
                         MATCH (n:base {id: node_id})
                         OPTIONAL MATCH (n:base)-[]->(connected:base)
                         RETURN node_id, connected.id AS connected_id
                       $$) AS (node_id text, connected_id text)""" % (
                self.graph_name,
                formatted_ids
            )

            incoming_query = """SELECT * FROM cypher('%s', $$
                         UNWIND [%s] AS node_id
                         MATCH (n:base {id: node_id})
                         OPTIONAL MATCH (n:base)<-[]-(connected:base)
                         RETURN node_id, connected.id AS connected_id
                       $$) AS (node_id text, connected_id text)""" % (
                self.graph_name,
                formatted_ids
            )

            outgoing_results = await self._query(outgoing_query)
            incoming_results = await self._query(incoming_query)

            for result in outgoing_results:
                if result["node_id"] and result["connected_id"]:
                    edges_norm[result["node_id"]].append(
                        (result["node_id"], result["connected_id"])
                    )

            for result in incoming_results:
                if result["node_id"] and result["connected_id"]:
                    edges_norm[result["node_id"]].append(
                        (result["connected_id"], result["node_id"])
                    )

        out: dict[str, list[tuple[str, str]]] = {}
        for orig in node_ids:
            n = self._normalize_node_id(orig)
            out[orig] = edges_norm.get(n, [])

        return out
    
    async def get_related_nodes(self, namespace, node_id: str, max_depth: int = 2, limit: int = 10) -> List[str]:
        """è·å–ä¸æŒ‡å®šèŠ‚ç‚¹ç›¸å…³çš„èŠ‚ç‚¹IDåˆ—è¡¨ï¼Œæ”¯æŒå¤šå±‚çº§æŸ¥è¯¢
        
        Args:
            node_id: è¦æŸ¥è¯¢å…³è”èŠ‚ç‚¹çš„èŠ‚ç‚¹ID
            max_depth: æœ€å¤§æŸ¥è¯¢æ·±åº¦ï¼Œé»˜è®¤ä¸º2
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼Œé»˜è®¤ä¸º10
            
        Returns:
            List[str]: å…³è”èŠ‚ç‚¹IDåˆ—è¡¨
        """
        if max_depth <= 0:
            return []
        
        # æ„å»ºå¤šå±‚çº§æŸ¥è¯¢çš„Cypherè¯­å¥
        # ä½¿ç”¨UNION ALLæ¥åˆå¹¶ä¸åŒæ·±åº¦çš„ç»“æœ
        depth_queries = []
        
        for depth in range(1, max_depth + 1):
            if depth == 1:
                # ç›´æ¥è¿æ¥çš„èŠ‚ç‚¹
                query = f"""
                    SELECT * FROM cypher('{self.graph_name}', $$
                        MATCH (n:base {{id: "{node_id}"}})
                        OPTIONAL MATCH (n)-[r*1..1]-(connected:base)
                        WHERE connected.id IS NOT NULL AND connected.id <> "{node_id}"
                        RETURN DISTINCT connected.id AS related_id
                    $$) AS (related_id text)
                """
            else:
                # å¤šå±‚çº§è¿æ¥çš„èŠ‚ç‚¹
                query = f"""
                    SELECT * FROM cypher('{self.graph_name}', $$
                        MATCH (n:base {{id: "{node_id}"}})
                        OPTIONAL MATCH (n)-[r*1..{depth}]-(connected:base)
                        WHERE connected.id IS NOT NULL AND connected.id <> "{node_id}"
                        RETURN DISTINCT connected.id AS related_id
                    $$) AS (related_id text)
                """
            depth_queries.append(query)
        
        # åˆå¹¶æ‰€æœ‰æ·±åº¦çš„æŸ¥è¯¢ç»“æœ
        combined_query = " UNION ALL ".join(depth_queries)
        final_query = f"""
            WITH all_related AS ({combined_query})
            SELECT DISTINCT related_id
            FROM all_related
            WHERE related_id IS NOT NULL
            LIMIT {limit}
        """
        
        try:
            results = await self._query(final_query)
            related_node_ids = []
            
            for result in results:
                if result.get("related_id"):
                    related_node_ids.append(result["related_id"])

            related_nodes = await self.get_nodes_batch(namespace, related_node_ids)
            logger.info(f"PGGraphStore|get_related_nodes|node_id={node_id}|max_depth={max_depth}|limit={limit}|related_node_ids={related_node_ids}|related_nodes={related_nodes}")

            return related_nodes
            
        except Exception as e:
            logger.error(f"Error getting related nodes for {node_id}: {e}")
            # å¦‚æœå¤šå±‚çº§æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°å•å±‚çº§æŸ¥è¯¢
            return await self._get_related_nodes_fallback(namespace, node_id, limit)
    
    async def _get_related_nodes_fallback(self, namespace, node_id: str, limit: int = 10) -> List[str]:
        """å›é€€æ–¹æ³•ï¼šè·å–ç›´æ¥å…³è”çš„èŠ‚ç‚¹"""
        try:
            edges = await self.get_node_edges(namespace=namespace, source_node_id=node_id)
            if not edges:
                return []
            
            related_node_ids = set()
            for source_id, target_id in edges:
                if source_id != node_id:
                    related_node_ids.add(source_id)
                if target_id != node_id:
                    related_node_ids.add(target_id)
            
            return list(related_node_ids)[:limit]
            
        except Exception as e:
            logger.error(f"Error in fallback method for {node_id}: {e}")
            return []

    def _normalize_node_id(self, node_id: str) -> str:
        """æ ‡å‡†åŒ–èŠ‚ç‚¹ID"""
        return str(node_id) if node_id is not None else ""

    async def test_connection(self) -> bool:
        """æµ‹è¯•æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸
        
        Returns:
            bool: è¿æ¥æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        """
        try:
            # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
            if self.db is None:
                await self.initialize()
            
            # æ£€æŸ¥è¿æ¥æ± æ˜¯å¦å­˜åœ¨
            if not self.db or not self.db.pool:
                logger.error("Database pool not available")
                return False
            
            # å°è¯•è·å–è¿æ¥å¹¶æ‰§è¡Œç®€å•æŸ¥è¯¢
            async with self.db.pool.acquire() as connection:
                # æ‰§è¡Œç®€å•çš„å¥åº·æ£€æŸ¥æŸ¥è¯¢
                result = await connection.fetchval("SELECT 1")
                if result == 1:
                    logger.debug("Database connection test passed")
                    return True
                else:
                    logger.error("Database connection test failed: unexpected result")
                    return False
                    
        except Exception as e:
            logger.error(f"Database connection test failed: {e}")
            return False